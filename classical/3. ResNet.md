# Deep Residual Learning for Image Recognition
> Author: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun  
> Journal: CVPR  
> Year: 2016 (ResNet was propoesd in 2015, but included by CVPR in 2016)  
> [Source paper link](https://arxiv.org/pdf/1512.03385.pdf)

- [Deep Residual Learning for Image Recognition](#deep-residual-learning-for-image-recognition)
  - [Abstract](#abstract)
  - [1. Introduction](#1-introduction)
  - [2. Related Work](#2-related-work)
  - [3. Deep Residual Learning](#3-deep-residual-learning)
    - [3.1. Residual Learning](#31-residual-learning)
    - [3.2. Identity Mapping by Shortcuts](#32-identity-mapping-by-shortcuts)
    - [3.3. Network Architectures](#33-network-architectures)
    - [3.4. Implementation](#34-implementation)
  - [4. Experiments](#4-experiments)
    - [4.1. ImageNet Classification](#41-imagenet-classification)
    - [4.2. CIFAR-10 and Analysis](#42-cifar-10-and-analysis)
    - [4.3. Object Detection on PASCAL and MS COCO](#43-object-detection-on-pascal-and-ms-coco)
  - [Appendix](#appendix)

## Abstract
ResNet helps to ease the training of very deep neural networks so it can take advantages of **depth bonus**.

## 1. Introduction
**Problem (What)**  
**Neural network's depth increasing** was once thwarted by the problem of vanishing/exploding gradients, which **had been largely addressed** by normalized initialization and intermediate normalization layers.  
While now the degradation problem baffles depth increasing again: with the network depth increasing, accuracy gets saturated and then degrades rapidly.

<img src=img/3_1.png>  

Figure 1. The degradation problem in DNN

**Solution (How)**  
Using **identity shortcut connections** to let the stacked nonlinear layers fit **residual mapping** of $F(x) := H(x) - x$, where $H(x)$ is the desired mapping of the whole residual block.

<img src=img/3_2.png width=40%>

Figure 2. Residual block.

**Contribution (Why)**
1. Solve the degradation problem and make DNN easier to optimize. 
2. Get substantially better SOTA results in many vision tasks, which suggest it is generic. Win the 1st places on: ImageNet classification, ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO 2015 competitions.

## 2. Related Work
1. Residual representations.
2. Shortcut connections.

## 3. Deep Residual Learning
### 3.1. Residual Learning
**Motivation (How does the author come to get the inspiration)**  
If the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counterpart. So the degradation problem suggests that the solvers
might **have difficulties in approximating identity mappings by multiple nonlinear layers**. With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.

### 3.2. Identity Mapping by Shortcuts

### 3.3. Network Architectures
**Plain Network**  
- Follow the philosophy of VGG nets: **preserve the time comlexity per layer**
  - For the same output feature map size, the layers have the same number of filters.
  - If the feature map size is halved, the number of filters is doubled so as to preserve the time complexity per layer.
- Use global average pooling instead of two fc layers, which makes the model much smaller.

**Residual Network**  
Add shortcut connections. When the dimensions increase, consider two options:
- Zero padding. The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. (This option introduces no extra parameter)
- Add a linear projection. $y = F(x, {W_i}) + W_sx$

<img src=img/3_3.png>

Figure 3. Example network architectures

### 3.4. Implementation
- **Preprocessing**
  - The image is resized with its shorter side randomly sampled in [256, 480] for scale augmentation.
  - Crop a 224Ã—224 patch randomly from original image or its horizontal flip.
  - Per-pixel mean subtracted.
  - Standard color augmentation in paper [21].
- **Architecture details**  
  - BN. Adopt batch normalization right after each convolution and before activation.
  - Without dropout.
- **Paramteter initialization**  
  Initialize the weights as in paper [13] and train all plain/residual nets from scratch.
- **Batch size**: 256
- **Optimization method**: SGD with momentum of 0.9 and weight decay of 0.0001.
- **Learning rate**: Starts from 0.1 and is divided by 10 when the error plateaus.
- **Iterations**: 600,000

## 4. Experiments
### 4.1. ImageNet Classification
- **Architecture**  
<img src=img/3_4.png>  
Table 1. Architectures for ImageNet.  
- Plain Networks $vs.$ Residual Networks  
<img src=img/3_5.png>  
Figure 4. Plain $vs.$ ResNet  
<img src=img/3_6.png width=40%>  
Table 2. Top-1 error (%, 10-crop testing) on ImageNet val set.

- **Identity $vs.$ Projection Shortcuts.**  
<img src=img/3_7.png width=40%>  
Table 3. Error rates (%, 10-crop testing) on ImageNet val set.  
(A) Zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter free;  
(B) Projection shortcuts are used for increasing dimensions, and other shortcuts are identity;  
(C) All shortcuts are projections.

- **Deeper Bottleneck Architectures.**  
  - ResNet50
  1. Replace each 2-layer block in ResNet34 with the 3-layer bottleneck block.
  2. Use option B for increasing dimensions.  

<div align=center>
<img src=img/3_8.png width=50%>  

Figure 5. Left: 2-layer block in ResNet34. Right: 3-layer bottleneck block in ResNet50.
</div>

  - ResNet101 & ResNet152: Using more 3-layer blocks.
  - Comparisons with State-of-the-art Methods.
<div align=center>
<img src=img/3_9.png width=50%>  

Table 4&5. Comparisons with SOTA methods.
</div>

### 4.2. CIFAR-10 and Analysis
- **Purpose**: Explore the behaviors of extremely deep networks.  
- **Train tricks**  
  Warm up. When it comes to 110-layer, the initial learning rate 0.1 is slightly too large to start converging. So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations).
- **Analysis of Layer Responses.**  
  The results support our basic motivation (Sec.3.1) that **the residual functions might be generally closer to zero than the non-residual functions**.

  <img src=img/3_10.png width=70%>  

  Figure 7. std of layer responses
- **Exploring Over 1000 layers.**  
  1202-layer network shows **no optimization difficulty**, since their tranining error < 0.1%. While its testing result is worse than 110-layer network, which may be triggered by **overfitting**.

### 4.3. Object Detection on PASCAL and MS COCO
<img src=img/3_11.png width=60%>  

## Appendix
A. Object Detection Baselines  
B. Object Detection Improvements  
C. ImageNet Localization
